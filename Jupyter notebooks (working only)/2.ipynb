{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87605ab5-2cf5-4b19-b1b8-ac52ce63a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba78435-97d5-4700-919d-3753b527adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = Image.open('data/coins.png')\n",
    "many_objects_1 = Image.open('data/many_objects_1.png')\n",
    "many_objects_2 = Image.open('data/many_objects_2.png')\n",
    "two_objects = Image.open('data/two_objects.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657e19f5-6be2-4df4-8ef6-5c63db37687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLabeledImage(gray_img: np.ndarray, threshold: float) -> np.ndarray:\n",
    "    '''\n",
    "    Generates a labeled image from a grayscale image by assigning unique labels to each connected component.\n",
    "    Arguments:\n",
    "        gray_img: grayscale image.\n",
    "        threshold: threshold for the grayscale image.\n",
    "    Returns:\n",
    "        labeled_img: the labeled image.\n",
    "    '''\n",
    "    \n",
    "    ### Solution - Challenge 1a - Devin Bresser ###\n",
    "    \n",
    "    # 1. Binarize the image with the given threshold\n",
    "    gray_img_array = np.array(gray_img)\n",
    "    gray_img_array_bin = (gray_img_array > threshold)\n",
    "\n",
    "    # 1b. Display image\n",
    "    plt.imshow(gray_img_array_bin, cmap='gray')\n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "\n",
    "    # 2. Segment the binarized image into connected regions\n",
    "    labeled_img = skimage.measure.label(gray_img_array_bin, background=0, connectivity=1)\n",
    "\n",
    "    # 2b. Display image\n",
    "    plt.imshow(labeled_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Annotate the labeled image with label numbers\n",
    "\n",
    "    # Recreate labeled image\n",
    "    plt.figure()\n",
    "    plt.imshow(labeled_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Compute object centroids (location where to put the labels)\n",
    "    object_labels = np.unique(labeled_img) # extract the sorted object labels\n",
    "    \n",
    "    for label in object_labels:\n",
    "\n",
    "        if label == 0:\n",
    "        # Special handling for background label\n",
    "        # Put it at the top right corner of the image to avoid conflicts\n",
    "            plt.text(labeled_img.shape[1]-10, 10, str(label), color=\"red\", ha=\"right\", va=\"top\", fontsize=26, weight='bold', clip_on=True)\n",
    "            continue\n",
    "            \n",
    "        rows, cols = np.where(labeled_img == label) # Separate the labeled image into subregions\n",
    "        centroid_row = np.mean(rows) # Compute centroid row\n",
    "        centroid_col = np.mean(cols) # and column\n",
    "        # Add the non-background labels to each centroid\n",
    "        plt.text(centroid_col, centroid_row, str(label), color=\"red\", ha=\"center\", va=\"center\", fontsize=\"20\", weight=\"bold\") \n",
    "\n",
    "    # To save as a file: \n",
    "    # plt.savefig('outputs/labeled_image.png')\n",
    "    plt.show()\n",
    "\n",
    "    return labeled_img\n",
    "        \n",
    "    ###\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78f3266-74a2-4e2e-a08e-607d3572347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute2DProperties(orig_img: np.ndarray, labeled_img: np.ndarray) ->  np.ndarray:\n",
    "    '''\n",
    "    Compute the 2D properties of each object in labeled image.\n",
    "    Arguments:\n",
    "        orig_img: the original image.\n",
    "        labeled_img: the labeled image.\n",
    "    Returns:\n",
    "        obj_db: the object database, where each row contains the properties\n",
    "            of one object.\n",
    "        out_img: the output image with the object properties annotated.\n",
    "    '''\n",
    "    ### Solution - Challenge 1b - Devin Bresser\n",
    "\n",
    "    # Store width and height of image\n",
    "    height, width = np.shape(orig_img)\n",
    "\n",
    "    # Initialize object DB\n",
    "    obj_db = []\n",
    "\n",
    "    # Extract unique objects (same code as part a)\n",
    "    object_labels = np.unique(labeled_img) # extract the sorted object labels\n",
    "\n",
    "    # Loop over each item\n",
    "    for label in object_labels:\n",
    "\n",
    "        if label == 0: \n",
    "            continue # do not process background\n",
    "            \n",
    "        obj_info = [label] # tuple requirement 1.\n",
    "        \n",
    "        # Compute centroid (same code from my solution for part a)\n",
    "        indices = np.argwhere(labeled_img == label) # Separate the labeled image into subregions\n",
    "        rows, cols = indices[:,0], indices[:,1] # extract rows and columns of each subregion\n",
    "        \n",
    "        centroid_row = np.mean(rows) # Compute centroid row\n",
    "        centroid_col = np.mean(cols) # and column\n",
    "        obj_info.append(centroid_row) # tuple requirement 2.\n",
    "        obj_info.append(centroid_col) # tuple requirement 3.\n",
    "\n",
    "        ## Compute minimum moment of inertia\n",
    "        \n",
    "        # Compute a, b, c using loops\n",
    "        a,b,c = 0,0,0\n",
    "        \n",
    "        # Compute a, b, and c per slides\n",
    "        a = np.sum((cols-centroid_col)**2)\n",
    "        b = 2*np.sum((rows-centroid_row)*(cols-centroid_col))\n",
    "        c = np.sum((rows-centroid_row)**2)\n",
    "\n",
    "        #print(f\"a: {a}, b: {b}, c: {c}\") # test\n",
    "\n",
    "        # Compute theta_1 and theta_2 per slides\n",
    "        theta_1 = 0.5*np.arctan2(b, a-c)\n",
    "        theta_2 = theta_1 + 0.5*np.pi\n",
    "        \n",
    "        # Compute E_min and E_max per slides\n",
    "        E1 = (a*np.sin(theta_1)**2 - b*np.sin(theta_1)*np.cos(theta_1) + c*np.cos(theta_1)**2)\n",
    "        E2 = (a*np.sin(theta_2)**2 - b*np.sin(theta_2)*np.cos(theta_2) + c*np.cos(theta_2)**2)\n",
    "        E_max = np.max([E1, E2])\n",
    "        E_min = np.min([E1, E2])\n",
    "    \n",
    "        # Compute orientation (a little ugly code but ok)\n",
    "        if(E_min == E1):\n",
    "            orientation = np.rad2deg(theta_1)\n",
    "        if(E_min == E2):\n",
    "            orientation = np.rad2deg(theta_2)\n",
    "        \n",
    "        # Compute roundness\n",
    "        roundness = E_min/E_max\n",
    "\n",
    "        # Append E_min, orientation and roundness to tuple (req. 4,5,6)\n",
    "        obj_info.append(E_min)\n",
    "        obj_info.append(orientation)\n",
    "        obj_info.append(roundness)\n",
    "\n",
    "        # I also want to store the ratio of E_min to the area of the object (for object classification in part C)\n",
    "        area = np.sum(labeled_img == label)\n",
    "        obj_info.append(E_min / area)\n",
    "\n",
    "        # One more property to store: a point 30px in the direction of orientation (for drawing lines)\n",
    "        orientation_rad = np.deg2rad(orientation)\n",
    "        line_endpoint_row = centroid_row + 30*np.sin(orientation_rad)\n",
    "        line_endpoint_col = centroid_col + 30*np.cos(orientation_rad)\n",
    "        obj_info.append(line_endpoint_row)\n",
    "        obj_info.append(line_endpoint_col)\n",
    "\n",
    "\n",
    "        # Append object tuple to database and repeat all of this for next object\n",
    "        obj_db.append(obj_info)\n",
    "\n",
    "    # Draw the dots and lines on the image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(orig_img, cmap='gray')\n",
    "    \n",
    "    for obj_info in obj_db:\n",
    "        _, centroid_row, centroid_col, _, _, _, _, line_endpoint_row, line_endpoint_col = obj_info\n",
    "        ax.plot(centroid_col, centroid_row, \"ro\") # red dot at centroid\n",
    "        ax.plot([centroid_col, line_endpoint_col], [centroid_row, line_endpoint_row], \"r-\") # red line in direction of orientation\n",
    "\n",
    "    plt.show()\n",
    "    return obj_db\n",
    "\n",
    "\n",
    "    ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd6122ac-e2a4-487d-8711-a018045e7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognizeObjects(orig_img: np.ndarray, labeled_img: np.ndarray, obj_db: np.ndarray, output_fn: str):\n",
    "    '''\n",
    "    Recognize the objects in the labeled image and save recognized objects to output_fn\n",
    "    Arguments:\n",
    "        orig_img: the original image.\n",
    "        labeled_img: the labeled image.\n",
    "        obj_db: the object database, where each row contains the properties \n",
    "            of one object.\n",
    "        output_fn: filename for saving output image with the objects recognized.\n",
    "    '''\n",
    "\n",
    "    # I am only using one feature for classification. It did a good job in my experiments.\n",
    "    # The feature is defined as the ratio of E_min to object area. See README for more information.\n",
    "\n",
    "    # The methodology to identify matches will be:\n",
    "        # 1. Extract objects and their features from the labeled input image using compute2DProperties\n",
    "        # 2. For each object, compare R = E_min / area.\n",
    "        # 3. If R_true / R_db > threshold, classify the object as a member of that class\n",
    "        # See README for information on threshold.\n",
    "    \n",
    "    input_features = compute2DProperties(orig_img, labeled_img)\n",
    "    R_input = [[feature[0], feature[6]] for feature in input_features] # Extract R from each item in the input feature\n",
    "    R_db = [[object[0], object[6]] for object in obj_db] # Extract R from each item in the database\n",
    "\n",
    "    # Now compare each input feature's value of R_input to all of those in R_db and stop if there is a match\n",
    "    threshold = 0.9\n",
    "    matches = []\n",
    "\n",
    "    # Iterate over input features\n",
    "    for input_feature in R_input:\n",
    "        input_label, R_input_value = input_feature\n",
    "\n",
    "        # Iterate over features in the object database\n",
    "        for db_feature in R_db:\n",
    "            db_label, R_db_value = db_feature\n",
    "            ratio = R_input_value / R_db_value\n",
    "            #print(f\"ratio: {ratio}\")\n",
    "\n",
    "            # Identify a match (0.9 < ratio < 1.1)\n",
    "            if ratio >= threshold and ratio <= 1/threshold:\n",
    "                matches.append([input_label, db_label]) # append the match to \"matches\"\n",
    "                break\n",
    "\n",
    "    matched_labels = [match[0] for match in matches]\n",
    "    input_features_filtered = [feature for feature in input_features if feature[0] in matched_labels]\n",
    "    \n",
    "    # Now that the input features are filtered to matches only, create output image\n",
    "    # This is very similar to Part b so I will reuse the code\n",
    "    # Draw the dots and lines on the image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(orig_img, cmap='gray')\n",
    "    \n",
    "    for input_feature in input_features_filtered:\n",
    "        _, centroid_row, centroid_col, _, _, _, _, line_endpoint_row, line_endpoint_col = input_feature\n",
    "        ax.plot(centroid_col, centroid_row, \"go\") # green dot at centroid\n",
    "        ax.plot([centroid_col, line_endpoint_col], [centroid_row, line_endpoint_row], \"g-\") # green line in direction of orientation\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_fn)\n",
    "    plt.show()\n",
    "           \n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "595bf40e-6d48-47d0-86c0-976882e15411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw2_challenge1a():\n",
    "    import matplotlib.cm as cm\n",
    "    from skimage.color import label2rgb\n",
    "    from hw2_challenge1 import generateLabeledImage\n",
    "    img_list = ['two_objects.png', 'many_objects_1.png', 'many_objects_2.png']\n",
    "    threshold_list = [130/256, 130/256, 130/256]   # You need to find the right thresholds\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "        orig_img = Image.open(f\"data/{img_list[i]}\")\n",
    "        orig_img = np.array(orig_img.convert('L')) / 255.\n",
    "        labeled_img = generateLabeledImage(orig_img, threshold_list[i])\n",
    "        Image.fromarray(labeled_img.astype(np.uint8)).save(\n",
    "            f'outputs/labeled_{img_list[i]}')\n",
    "        \n",
    "        cmap = np.array(cm.get_cmap('Set1').colors)\n",
    "        rgb_img = label2rgb(labeled_img, colors=cmap, bg_label=0)\n",
    "        Image.fromarray((rgb_img * 255).astype(np.uint8)).save(\n",
    "            f'outputs/rgb_labeled_{img_list[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "375c1cba-fa2f-40d4-bc51-8d34ae674211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw2_challenge1b():\n",
    "    labeled_two_obj = Image.open('outputs/labeled_two_objects.png')\n",
    "    labeled_two_obj = np.array(labeled_two_obj)\n",
    "    orig_img = Image.open('data/two_objects.png')\n",
    "    orig_img = np.array(orig_img.convert('L')) / 255.\n",
    "    obj_db  = compute2DProperties(orig_img, labeled_two_obj)\n",
    "    np.save('outputs/obj_db.npy', obj_db)\n",
    "    print(obj_db)\n",
    "    \n",
    "    # TODO: Plot the position and orientation of the objects\n",
    "    # Use a dot or star to annotate the position and a short line segment originating from the dot for orientation\n",
    "    # Refer to demoTricksFun.py for examples to draw dots and lines. \n",
    "\n",
    "    ### Comment: I implemented this within the function compute2DProperties. Please refer to that function for my implementation. ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e0bf345-35aa-4efc-aa53-95c988eefeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw2_challenge1c():\n",
    "    obj_db = np.load('outputs/obj_db.npy')\n",
    "    img_list = ['many_objects_1.png', 'many_objects_2.png']\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "        labeled_img = Image.open(f'outputs/labeled_{img_list[i]}')\n",
    "        labeled_img = np.array(labeled_img)\n",
    "        orig_img = Image.open(f\"data/{img_list[i]}\")\n",
    "        orig_img = np.array(orig_img.convert('L')) / 255.\n",
    "\n",
    "        recognizeObjects(orig_img, labeled_img, obj_db,\n",
    "                         f'outputs/testing1c_{img_list[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8068fe7-eaa0-4a31-b7fd-694e618747bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devin\\AppData\\Local\\Temp\\ipykernel_16280\\2987045862.py:15: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = np.array(cm.get_cmap('Set1').colors)\n"
     ]
    }
   ],
   "source": [
    "hw2_challenge1a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59cc5848-2e94-4566-b08b-2d8ba5e1691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 263.6188664392548, 349.43466281815796, 3869865.491829468, 72.4213694049478, 0.534509794456744, 507.7231031001663, 292.2179676990492, 358.4950936650894], [2, 256.6118811881188, 195.30247524752474, 359632.9523289283, -39.37818679773083, 0.4810493516073533, 178.03611501432093, 237.5787928447855, 218.49173026175214]]\n"
     ]
    }
   ],
   "source": [
    "hw2_challenge1b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c8221aa-dfa8-4536-8825-a0d6d71e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw2_challenge1c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac850e1-5ed7-4636-b822-51bef09a569f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
